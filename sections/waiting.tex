%!TEX root = ../main.tex

\section{Waiting Time}
\label{sec:waitingTime}
\mk{Need an alg for the IP tree}

The waiting time function is used to calculate the total time advertisers have to wait before being admitted to the ad cache. 
The function directly shapes the structure of the ad cache, determines its diversity and performs flow control. 
It also protects against attacks, where a malicious actor tries to dominate the ad cache and exhaust the resources of a registrar. 

Each request is given a waiting time based on the IP address of the advertiser, the advertised service, and the current state of the ad cache.
The waiting time function is divided into two parts: an $\algvar{occupancy\_score}$ (ranging from $0$ to $\infty$) and a $\algvar{similarity\_score}$ (ranging from $0$ to $2$) and is normalized by the amount of time each ad spent in the cache $E$ (\ie \emph{expiry time}). $E$ determines the absolute values of the returned waiting time. \er{I am not sure to understand the previous sentence (before reading the rest of the section)} The final result is a product of all three: $w = E \times \algvar{occupancy\_score} \times \algvar{similarity\_score}.$

The $\algvar{occupancy\_score}$ is based uniquely on the number of ads already in the cache.
Its role is to progressively increase the waiting time as the ad cache fills up and to limit the memory used by a registrar $\algvar{occupancy\_score} = \frac{1}{(1-\frac{d}{n})^{P_\textit{occ}}}$,
where $d$ is the number of ads in the cache, $C$ is the capacity of the cache, and $P_\textit{occ}$ is a protocol parameter. 
When the number of ads in the cache is low ($d \ll  C$ ), the $\algvar{occupancy\_score}$ goes to $1$. 
As the ad cache fills up, the score will be amplified by the divisor of the equation. 
The higher the value of $P_\textit{occ}$, the faster the increase. 
With a current occupancy $d$ close to the capacity $C$, the $\algvar{occupancy\_score}$ goes to infinity thus limiting the number of admitted requests.
We analyze the behavior of the waiting function and choose optimal system parameter values in \Cref{sec:analysis}.

The role of the $\algvar{similarity\_score}$ is to determine how similar the incoming request is to the ads already in the ad cache in terms of IP address and service. 
Requests significantly different from the current content of the cache receive lower similarity scores resulting in lower overall waiting times. 
Such an approach promotes fairness across services (\ie, it is easier for less popular services to get into the cache) and protects against attempts to fill the ad cache by a small number of advertisers (as identified by their IP addresses). The similarity score is defined as the sum of the similarity score for the IP, the similarity score for the service of the request, and a system parameter $G$: $\textit{similarity} = G + \textit{similarity(service)} + \textit{similarity(IP)}$. \er{not sure it is the best to name a global value (similarity) the same as two different functions. Maybe consider using different names, e.g., sim\_service and sim\_IP?}

$G$ ensures that the waiting time never reaches $0$ even when requests get $0$ values for IP and service similarity score. Together with $P_\textit{occ}$, it shapes the behavior of the waiting functions. We choose values for those parameters in \Cref{sec:analysis}.

The similarity score for services is given by $\textit{similarity(service)}= \frac{d(\textit{service})}{d}$.
Here, $d(\textit{service})$ is the number of ads for the specified service already in the cache and $d$ is the total number of ads in the cache. 
The score goes to $1$ as the specified service dominates the cache $d(\textit{service}) \approx d$. 

A simple similarity score used for services cannot be securely applied to IP addresses. 
An attacker may be able to generate a large number of different addresses sharing the same prefix (\eg using a single /24 IPv4 network) that, while similar, would receive low \emph{similarity scores}.
A common solution (\eg, as adopted by the Go Ethereum client~\cite{geth,marcus2018low}) is to limit the number of IP addresses coming from the same (\eg \ /24 IPv4 address) network.
However, it is impossible to reliably set those limits without knowledge about the network size or NAT configuration of the honest nodes. 
Instead, we propose a more versatile approach that directly captures the similarity level across different IPs and translates it into a numerical score. 

We introduce a binary \emph{tree}, as shown on \Cref{fig:ip_tree}, that stores IP addresses used in the existing registrations in the ad cache.
Each vertex stores a counter, while the edges represent consecutive $0$s or $1$s in a binary representation of IP addresses.
For simplicity, we present the \emph{tree} for IPv4 addresses but its adaptation for IPv6 is straightforward.

\begin{figure}
    \centering
    \includegraphics[width=0.40\textwidth]{img/ip_tree_small}
    \vspace{-0.05in}
    \caption{Inserting an IP address into the IP \emph{tree} structure.}
    \label{fig:ip_tree}
    \vspace{-0.15in}
\end{figure}

Apart from its root, the \emph{tree} consists of 32 levels (33 levels in total) representing bits in the binary representation of IPv4 IP addresses. 
The root level is depicted as level $0$, the level of its successor as level $1$ and so on. 
The counter of every \emph{tree} node is initially set to $0$. When adding an IP to the \emph{tree}, the address is first converted to its binary representation and follows a path in the \emph{tree} corresponding to consecutive bits. 
Counters of all the visited nodes are increased by $1$. Removing an IP from the \emph{tree} follows the analogical procedure but decreases all the counters on the path. 
As a result, the root counter stores the number of all the IP addresses in the ad cache, its $0$ successor stores the number of the IP addresses starting with $0$, root's $1$ successor stores the number of the IP addresses starting with $1$ and so on. 

After each addition of an address to the \emph{tree}, an IP score is calculated.
The score is a sum of \emph{penalty points} obtained on visited nodes. 
$$\textit{score}(IP)=\sum_{i=1}^{32} \textit{penalty}(p_{\geq i}) $$
where $p_{\geq i}$ is the number of IP addresses in the cache sharing a prefix with $IP$ with a length of at least $i$. A penalty point is given at $p_{\geq i}$ if the IP address to be added makes the tree more unbalanced than the tree currently is:

\begin{equation}
    \textit{penalty}(p_{\geq i})=
    \begin{cases}
      0, & \text{if}\ p_{\geq i} \leq \frac{p_0}{2^i} \\
      1, & \text{otherwise}
    \end{cases}
  \end{equation}

The counter values are taken \emph{before} the increment caused by adding the address\footnote{The first added address will thus always have a score of $0$}. 
Finally, the similarity score for an IP is normalized by the length of the IP address (and thus the maximum possible number of penalty points): $\textit{similarity(IP}) = \frac{\textit{score(IP)}}{32}$

Similar to the service score, the IP similarity score ranges from $0$ to $1$ and returns values closer to 1 for IP addresses sharing the same prefix (the longer the shared prefix, the higher the score).

The final formula for the waiting time function can be represented with the following formula:%, adding all \emph{similarity scores} and multiplying by the $\algvar{occupancy\_score}$:

\begin{equation}
\begin{split}
    \textit{w(IP, service)} = 
    E(G + \frac{d(\textit{service})}{d} + \frac{\textit{score(IP)}}{32}
    )
    \frac{1}{(1-\frac{d}{n})^{P_\textit{occ}}}
\end{split}
\end{equation}

%The formula can be simplified like in equation~\ref{eq:simp}, where ss determines the the $\algvar{similarity\_score}$ and os the $\algvar{occupancy\_score}$.

%\begin{equation}
%\label{eq:simp}
%    \textit{w(IP, service)} = 
%    (\textit{ss(IP)} + 
%    \textit{ss(service)})
%    \textit{os()}
%\end{equation}
\begin{algorithm}[]%
    \caption{%
        Adding an address to the IP tree.
    }%
    \label{alg:tree}%
    \begin{algorithmic}[1]%
         \footnotesize
        \Procedure{ad}{$\algvar{tree}$, $\algvar{IP}$}
            \State $v \gets \algvar{tree.root}$
            \State $\algvar{score} \gets 0$
            \State $\algvar{bits} \gets \algvar{IP.to\_binary()}$
            \For{$i$ in $0, 1,..., 31$}
                \State $v\algvar{.counter} \gets v\algvar{.counter} + 1$
                \If{$\algvar{bits}[i] = 0$}
                    \State $v \gets v\algvar{.left}$
                \Else
                    \State $v \gets v\algvar{.right}$
                \EndIf
                \If{$v\algvar{.counter} > \frac{\algvar{tree.root.counter}}{2^i}$}%
                    \State $\algvar{score} \gets \algvar{score} + 1$
                \EndIf
            \EndFor
            \State \Return $\algvar{\frac{score}{32}}$
        \EndProcedure
    \end{algorithmic}%
\end{algorithm}%\

\subsection{Lower Bound}
With the waiting time formula, every change in the ads stored in the ad cache may increase or decrease the waiting times of other pending requests. 
Therefore, an advertiser receiving waiting time $w_{t1}$ at time $t1$, may get a smaller waiting time $w_{t2}$ at time $t2$ ($t1 < t2$) in case the content of the ad cache is different (\eg when an ad for the same service expires between $t1$ and $t2$). 
As a result, advertisers are incentivized to keep checking the waiting times as frequently as possible hoping for a better one and generating unnecessary overhead in the system.
%Registration ticket requests should be kept to the minimum and an incentive for constantly spamming ticket requests to get a better waiting time can overload a network and can lead to some nodes getting better performance than the rest.
To prevent this behavior, we design a mechanism ensuring that any node already in possession of a ticket with a determined waiting time, cannot get a better waiting time (including the new waiting time and the time passed between the first ticket request and the subsequent) by sending new ticket requests.
One solution to this problem is to take into account all the expiration times when calculating the waiting time. 
However, it is computationally expensive (\eg $O(d)$ per request) and unfeasible in practice as it opens a possibility of denial of service attacks. \er{I am not sure to see why and how?}\mk{To do that we'd need to iterate through (potentially all the) registrations that we have in our cache every time we receive a request. An attacker could then send dummy requests just to force the registrar to use its CPU.}

\begin{figure}
    \includegraphics[width=0.45\textwidth]{img/lower_bound.png}
    \vspace{-0.05in}
    \caption{Waiting time lower bound.}
    \label{fig:lower_bound}
    \vspace{-0.15in}
\end{figure}

When asking for a new waiting time before the previously obtained one elapses,
an advertiser loses its already accumulated waiting time (including the previous ticket allows the registrar to ignore the request). This means that asking for a new waiting at time $t2$ can lower the overall waiting only if the new waiting time $w_{t2}$ is smaller than $w_{t1}$ by more than the time elapsed $t2 - t1$: $w_{t1} - w_{t2} < t2 - t1$.
To ensure this is not the case, our protocol enforces a lower bound on the
waiting time, \ie, we make sure that an advertiser's waiting time received at
$t2$ is not smaller than the waiting time at $t1$ ($t1 < t2$) by more than
$t2 - t1$ (\Cref{fig:lower_bound}).
However, holding such a bound for every request (\ie, every combination of IP/service) would cause significant memory overhead ($O(|\textit{IPs}|\times|\textit{services}|) \gg O(d)$) and would present an easy way for an attacker to create additional state at the registrar. 

To store the lower bound more efficiently, we rewrite the waiting formula as a sum:
\begin{equation}
\begin{split}
    \textit{w(IP, service)} = 
    \frac{E\cdot G}{(1-\frac{d}{C})^{P_\textit{occ}}} + \frac{E\cdot d(\textit{service})}{d(1-\frac{d}{C})^{P_\textit{occ}}} + \frac{\textit{score(IP)}}{32(1-\frac{d}{C})^{P_\textit{occ}}}
\end{split}
\end{equation}


Ensuring that the lower bound is enforced for each of the three components, implies that the total waiting will respect the lower bound as well. At the same time, it only requires storing the lower bound for every IP/service in the table and not all their combinations. This approach reduces the memory overhead to $O(d(IPs)+d(services)) = O(d)$.

\sysname keeps a lower bound for each IP and service in the ad cache. When a specific service enters the cache for the first time, bound(service) is set to 0 and a timestamp(service) is set to the current time. When a ticket request arrives for the same service, we calculate the service waiting time $w_{\textit{service}}$ and return the value, $w_{\textit{service}} = \textit{max}(w_{\textit{service}}, \textit{bound}(\textit{service}) - \textit{timestamp}(\textit{service}))$. The bound and the timestamp are updated when a new ticket is issued and $w_{\textit{service}} > (\textit{bound}(\textit{service}) - \textit{timestamp}(\textit{service}))$. 

We also maintain lower-bound state for the ticket holders' IP addresses in the IP tree structure: the state for an IP address is maintained at the node, which corresponds to the longest prefix match in the existing tree (without introducing new nodes). We also aggregate the lower bound states of multiple IPs mapping to the same node by applying a max() function.  
%\hl{The same holds for IPs.}
%\mk{TODO: @Onur we need to introduce the waiting time in the tree I believe}


