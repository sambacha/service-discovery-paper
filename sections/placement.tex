%!TEX root = ../main.tex
%=========================================================

\section{Ads placement}
\label{sec:placement}
In the following, we provide a description of distributing advertisement in the network and discuss the process of search and establishing sub-protocol connections. 


\subsection{Data Structures}\label{sec:struct}
We start by describing data structures necessary for topic-specific registrations and lookups. 

\para{Registration Table}
In order to execute the ad distribution process described below,  each advertiser maintains a per-topic \emph{registration table} for each advertised topic. The table keeps track of the ongoing registration attempts with different registrars.  The \emph{registration table} is similar to the routing table used in Kademlia protocol (as described in \Cref{sec:background}). It stores advertiser's peers divided into k-buckets. However, instead of placing nodes in buckets based on the distance from the advertiser, it uses the distance from the topic ID. I.e. the \emph{registration table} stores $K_\textit{register}$ potential registrars for every distance range (bucket) from the topic ID (\Cref{fig:ticket_table}). When created, the table is initialized with peers from the routing table. 

\dk{Imo, it is not clear if there is a single topic table each node maintains, or if each node has one topic table for each Topic ID. Especially because the previous Section said there is a \emph{second} table for topics.}

\begin{figure}
    \includegraphics[width=0.45\textwidth]{img/ticket_table}
    \caption{Creation of ticket table from the routing table.}
    \label{fig:ticket_table}
 \end{figure}

\para{Search Table}
%The ad lookup process is supported by a \emph{search table}. 
%Searchers maintain a separate table per topic they are currently looking for. 
%\ie for each topic the client wants to start discovering nodes, a new \emph{search table} is created. 
%Similar to the \emph{registration table}, the \emph{search table} also stores k-buckets of registrar nodes by distance to the topic hash and buckets are initially filled from the local routing table organised by the distance from the topic hash.

The ad lookup process is supported by a \emph{search table}. 
Searchers maintain a separate table per topic they are currently looking for. 
\ie for each topic the client wants to start discovering nodes, a new \emph{search table} is created. 
Similar to the \emph{registration table}, the \emph{search table} also stores k-buckets of registrar nodes by distance to the topic hash and buckets are initially filled from the local routing table organised by the distance from the topic hash.
\emph{Search table} buckets are also filled with new nodes discovered during the registration process.
The \emph{k} factor of the search table should be relatively large in order to make the search efficient. 
By default we use \emph{k=16} similarly to the Kademlia DHT. 
Also, different nodes using the same /24 domain IP address can not be added to the table to avoid sybil attacks.
Ideally, all k-buckets should be constantly full, making it possible to query any distance to the topic hash. 
There is also a refresh process, similar to the Kademlia DHT table, where periodically a random bucket is checked for empty buckets. The default refresh time used is 100 seconds. 
When an empty bucket is detected a random lookup can be performed to discover new nodes.
Also, the last node in the bucket is pinged to check it is still alive. 
In case it is not, it is removed from the table.

%and a new 'search table' is created for each topic. 
%The bucket size \texttt{k}  of the search table should be relatively large in order to make the search efficient. 
%By default we use\texttt{k=16},  similarly to the local routing table (Kademlia DHT).
%Tickets are not required for search and nodes can not be added multiple times in the same k-bucket.

\subsection{Distributing ads across registrars}\label{sec:registration_multi}
When an advertiser wants to associate themselves with a topic, they start by creating a topic-specific \emph{registration table} (as described above). 
Every peer present in the \emph{registration table} is a potential registrar. 
The objective of the ad placement process is to continuously maintain $K_\textit{register}$ active (\ie unexpired) registrations in every bucket. 
Buckets located close to the topic hash cover less hash space than buckets located further away and, in turn, contain less potential registrars. 
Placing a fixed amount of ads per bucket, make registrars close to a topic hash more likely to receive registrations for that specific topic. 
Increasing $K_\textit{register}$, makes the advertiser easier to find at the cost of increased communication overhead. More formally, the probability that a registrar has a topic-specific ad, based on the number of the nodes in a bucket $N_\textit{nodes}$, and the number of advertisers $N_\textit{advertisers}$ is given by:

\begin{equation}
    P_\textit{have ad} = 1-(1 - \frac{K_\textit{register}}{N_\textit{nodes}})^{N_\textit{advertisers}}
\end{equation}


For every bucket in the \emph{registration table}, the advertiser selects
$K_{register}$ random peers and attempts to perform a registration.
We describe details on the admission procedure in \Cref{sec:registration}. 
A successful registration places an ad on an advertiser for a fixed amount of time $t_\textit{lifetime}$.
If a registration is unsuccessful (the selected registrar is down or refuses to store the ad), the advertiser selects another random peer from the same bucket and retries the registration process. 
The advertisers always maintain $K_\textit{register}$ attempts and/or
successful registrations per bucket unless there are less than $K_\textit{register}$ peers present in a specific bucket.
The advertiser repeats the process for every bucket in the \emph{registration table}. 
\michal{An algorithm here?}

 




The \emph{registration table} is initialized with the peers already present in the routing table. It is thus possible that an advertiser will not know any nodes in buckets located close to the topic hash\footnote{This usually happens when the advertiser's ID is distant from the topic hash.}. 
To fill the empty buckets, the advertiser asks potential registrars to return $N$ closest peers to the topic hash they know of. 
The procedure is similar to the regular DHT \emph{FIND\_NODE} operation described in \Cref{sec:background}. 
The registrars respond with a list peers regardless of the success of the registration operation. 
The advertiser uses the returned information to populate its \emph{registration table}. 
As the advertiser progresses through the buckets, it queries potential registrars located closer to the topic hash and thus having a more detailed view of this part of the network. 
Similarly to the DHT routing, the registration procedure is guaranteed to find the closest node to the topic hash in the network. 
\ramin{Why not just use FIND\_NODE?}
\michal{It's basically FIND\_NODE but based on a different table (\emph{registration table}, rather than the routing table) and performing the registration on top of traversing the DHT. IMO it's easier to have a separate name here.}

%In addition to that, every time a node sends a ticket or registration request, the registrar replies with the closest nodes to 'the topic hash' that it knows. 
%This helps filling up k-buckets without sending additional lookups. 
%Also, when performing topic search (sending lookups for specific topics),  closest known nodes to 'the topic hash' are attached by the registrar node in the response.




\subsection{Lookup operation}\label{sec:lookup}

To find ads, \sysname uses a process similar to the registration procedure. 
%The goal is to find $N_\textit{lookup}$ node advertised with a specific topic. 
%Each lookup requires a topic-specific \emph{search table} initially populated with nodes from the \emph{routing table}. 
Each lookup requires a topic-specific \emph{search table} described in \Cref{sec:struct}
The searcher progressively moves through buckets (starting from the furthest away), randomly chooses $L_\textit{lookup}$ registrars per buckets and sends them parallel lookup requests. 
Once received results from the first $L_\textit{lookup}$ selected registrars,  other $L_\textit{lookup}$ registrars are selected from the  following bucket with a smaller distance to the topic id.
The queried nodes are removed from the bucket.
Searchers stop the lookup procedure when they discover $N_\textit{lookup}$ peers. This is likely to happen far before reaching registrars located near the topic hash, especially for popular topics. As a result, \sysname avoids hot-spots in the network and ensure fair load distribution. 
In case all buckets have been queried without finding enough nodes, the lookup process continues starting with the furthest distance bucket again. 
Once the lookup process is completed, the nodes are returned to the application.
%  but only in case there are nodes  discovered from different nodes and different buckets,  otherwise no nodes are returned to the application to avoid sybil attacks.

Since furthest buckets are also the largest (\ie it contains 50\% of all the honest registrars), an attacker placing malicious registrars in those buckets would require significant resources to eclipse the procedure. 
Searchers progressively moving towards smaller buckets near the topic hash guarantees successful discovery even for unpopular topics.
Each queried registrar responds with a list of $N_\textit{return}$ topic-specific advertisers the registrar know of. 
If the total number of topic-specific registrations in the table is larger then
$N_\textit{return}$, the registrar should return a random subset.
While both $N_\textit{return}$ and $N_\textit{lookup}$ are protocol parameters,
the amount of ads returned by a single registrar must be lower than the total
number of ads searchers are aiming to find $N_\textit{return} <
N_\textit{lookup}$. This is to diversify sources of ads received by the searcher. I.e., a single malicious registrar is not able to stop an honest searcher from contacting other nodes.
%\sergi{And is possible also a good idea to use a $N_\textit{lookup}$ parameter that forces that results from multiple buckets are used}
There is a trade-off between overhead and security when choosing $N_\textit{return}$ and $N_\textit{lookup}$. 
By requiring a large amount of total ads to stop the search
($N_\textit{lookup}$) compared with the  ads returned by registrar
$N_\textit{return}$ a higher diversity of data sources is achieved at the cost of contacting a large number of registrars. On the other hand,  similar values of both $N_\textit{lookup}$ and $N_\textit{return}$ reduce the overhead but increase the danger of a searcher receiving ads uniquely from malicious nodes. Finally,  low values of $N_\textit{lookup}$ stop the search operation early,  before reaching registrars close to the topic hash, contributing to more equal load spread.
However,  for security reasons it is always recommendable to use $L_\textit{lookup}$, $N_\textit{return}$ and $N_\textit{lookup}$ parameters to ensure that at least 3 different registrars are queried from 2 different distance buckets.
In the evaluation in \cref{sec:eval},  we set $L_\textit{lookup}=1$, $N_\textit{return}=10$ and $N_\textit{lookup}=30$, to be able to get results from 3 different registrars from 3 different buckets.
\sr{Future work: investigate the impact of search parameters}

%\michal{For security, highlight that we want to mix results from different buckets and from different registrars - this is very important for security}
\michal{Consider renaming search/registration tables into search/registration caches}
%\michal{From Felix: 1) how exactly we choose the registrars to ask 2) filling the search table as you go 3) how many queries per node, how do you combine the results 4) when do you stop}
\ramin{In practice, is $N_\textit{lookup}$ a parameter that can be controlled? It sounds like something application-specific, e.g. for some applications, it might be enough for a searcher to just find one node to be able to use the application as intended. Search for 10 and choose one randomly from them?}
\michal{That's a good point. Not sure if we should allow the application to control both $N_\textit{lookup}$ and $N_\textit{return}$. Or maybe fix a ration between both and automatically set $N_\textit{return}$ once $N_\textit{lookup}$ is chosen by the application.}
\sergi{I think $N_\textit{return}$ shouldnt be configurable since it is limited by the maximum number of nodes returned in a message which i think is 16.  I think $N_\textit{lookup}$ can be configurable, but never smaller than a value multiple of $N_\textit{return}$}

%\michal{This is very vague and probably not something we should demand from the application. Either we provide a mixing/filtering feature ourselves, or we don't mention it.}
%Once ads are discovered,  the searcher can initiate topic-specific connections based on rules specified by the application. 
%Applications do not only receive a list of potential nodes to connect but also the registrar node source of the search response.
%In order to avoid eclipse attacks it is recommended not to start connections with nodes coming from the same registrar. 
%Therefore connections are initiated always trying to keep the maximum diversity of the  registrars nodes used.
%Once all nodes received have been used to try new connections,  a new search process is initiated. 
%\michal{Either we provide good results by ourselves or we don't talk about it}


%\subsubsection{Updating connections}
%\michal{Didn't touch this subsection. Should it be here?}
%\sergi{i integrated some part in the last paragrah and the resti is already in the background section}
%Using the search results, a searcher updates its subprotocol-specific connections. 
%For instance,  each Ethereum client keeps connections towards 50 other nodes,  2/3 of those are for inbound connections (\ie 34 connections) where the client has no control over them, and the rest (about 16 connections) are four the outbound connections initiated towards the discovered nodes using the discovery protocol. 
%When a lookup is done,  the nodes discovered are stored in a buffer. 
%For each node of the buffer a connection is initiated. 
%The connection can be successful when the node targeted is alive and has empty slots in the inbound connections,  otherwise the connection is unsuccessful and a new node is selected.
%The nodes from the buffer are consumed as connections are attempted to the nodes.
%The buffer not only stores the nodes information but also the registrar node that replied with it.
%In order to avoid eclipse attacks it is recommended not to start connections with nodes coming from the same registrar. 
%Therefore connections are initiated always trying to keep the maximum diversity of the  registrars nodes used.

%\onur{need to explain this process since security depends on that.}

%All the modifiers from the first part of the equation increase with increasing number of the same items that are already in the table, i.e., reduction in diversity. Thus it's getting increasingly difficult to register ads for the same IP/ID/topic. For instance, ads for less popular topic will receive lower waiting times than popular ones. Note that the table does not prevent anyone from registering, but rather makes it slower for already popular items. Such a mechanism promotes diversity in the table and protects against Sybil attacks so that an attacker who is in control of a limited pool of IP addresses won't be able to dominate the table with many ads. The low exponent for the topics is motivated by the topics in the network that are likely to follow a skewed (e.g., a zipf-like) distribution. In contrast, honest nodes' IPs/IDs should follow a uniform distribution.

%The latter part of the formula is determined based on a multiple of ad-lifetime and the current utilisation (i.e., occupancy divided by capacity) of the table. When the utilisation becomes closer to 1.0, the base time becomes very large due to a very small denominator. Before the waiting time becomes infinite (when utilisation becomes 1), the waiting time becomes extremely high, in which case the advertisers give up as explained in the ad distribution process.

%The advertiser attempts to place an ad on each registrar and keeps the latest ticket issued by that registrar. It also keeps references to all pending tickets in a priority queue keyed by the expiry time of the ticket so it can efficiently access the next ticket for which a placement attempt is due.
%Since the table is limited to K values per bucket,  this means there are K on-going registrations per distance to the topic hash id. 
%The number of buckets used in the ticket table is a configurable parameter, but it is recommended to use small number of buckets,  instead of using a bucket per bit of the address space,  to avoid overloading registrars with IDs close to the topic hash. 
%Any node with a distance below the maximum distance minus the number of buckets will be stored in the same bucket.
%In our evaluation we used a ticket table with 10 buckets. 



%This objective is achieved by the advertisers setting a timer with a duration of ad-lifetime immediately upon the receipt of a REGCONFIRMATION from a node in a bucket b, and once the timer expires (after ad-lifetime passes) the advertiser starts a fresh registration with a node that is also located in bucket b. 
%The ticket table is used to store the tickets obtained for each on-going registrations and to keep track of the expiration times of active registrations.

%\subsubsection{Bucket refresh}

%The Ticket table needs to be initialised and refreshed to fill up all the per-distance k-buckets. 
%Ideally,  all k-buckets should be constantly full,  meaning that the advertisers place registrations at registrars in all distances to the topic hash. 
%An option to fill up all k-buckets would be to send periodic lookups for the specific distance to the topic hash, but since there are some distances that tend to be empty in the id space,  sending periodic lookups for the topic hash may create an additional overhead that can be too expensive and create too much traffic in the network. To avoid that, initially, the 'ticket table' k-buckets are filled performing local Ethereum routing table lookups to all distances to the 'topic hash' of the advertised topic.


%\michal{The below seems like an implementation detail.}
%There is  also a refresh bucket process,  similar to the Kademlia DHT table,  where periodically a random bucket is checked to see if it is empty.
%The \texttt{refresh time} is a configurable parameter. 
%We set a \texttt{refresh time=10 seconds} as a reference for our performance evaluation.
%During the refresh process,  in case the bucket checked is empty, 
%a local lookup to the Kademlia DHT table is performed. 
%In case no nodes have been found for the bucket distance,  a Kademlia lookup is performed towards the topic hash~\footnote{The lookup is performed by sending a FINDNODE message,  as described inhttps://github.com/ethereum/devp2p/blob/master/discv4.md.}
%Also, all nodes in the checked bucket are pinged to check they are still alive. 
%In case they are not, tickets for those dead nodes are removed from the ticket table and registrations to new nodes are initiated.




%We implemented and evaluated a lookup strategy that choose which nodes from which buckets ask first trying to diversify the nodes discovered as much as possible and trying not to overload nodes close the topic hash.
%A random node is picked from a bucket following a round-robin approach. 
%It starts picking a random node from the highest distance bucket and follows to the next distance in the bucket list.
%In order to find new results,  bucket entries are replaced when the node fails to answer or when it answers with an empty list of ads. 

%There is also a refresh process,  equivalent to the \emph{ticket table} refresh process, where random buckets are selected and all nodes in the buckets are checked for liveness.
%In case the bucket is empty a Kademlia lookup is performed towards the topic hash.
