%!TEX root = ../main.tex
%=========================================================

%Background
% -> I would present Kademlia directly rather than a generic DHT model (which we can assume is common knowledge for the readers) -- we only want to summarize what operations it supports and directly explain how Kademlia implements it.
% -> The DHT description is both very general (concepts without explaining the graph structures) and very specific (names of the messages exchanged between the nodes).\include{sections/new_background.tex}
% -> 2.2 why present the Ethereum network as similar to a DHT but not writing it is one? This is confusing imho
% -> What is a subprotocol in the context of 2.2?
% -> It seems to me that this background section should be remodeled into a problem statement that focuses on how Ethereum does service discovery and identifies clearly the issues with scalability and attacks.

\section{Background}
\label{sec:background}

We start by detailing the operation of Ethereum's global DHT, which is an evolution of the canonical Kademlia~\cite{maymounkov2002kademlia} DHT.
Then, we present the current service discovery mechanism, part of Ethereum's \discv series of protocols operating atop this DHT, and discuss its shortcomings.

\subsection{The Ethereum global DHT}
\label{sec:background:dht}

All nodes in the Ethereum ecosystem participate in a global distributed hash table (DHT).
A DHT is a structured overlay network allowing lookup operations, \ie locating a node or group of nodes in charge of a particular \emph{key} in a target \emph{key space}~\cite{chord,rowstron2001pastry}. Ethereum uses an evolution of Kademlia~\cite{maymounkov2002kademlia}, a robust and mature DHT design. % as its global DHT.
Every node in the overlay is assigned a unique identifier (\emph{node ID}), drawn from the same key space as items, i.e., information stored in the DHT. %\mk{we haven't mention item storage before}
The Kademlia DHT assigns a specific key $i$ to the $k$ \emph{closest} node to $i$ in that space where $k$ is a system parameter.
This distance $d$ between two keys $x$ and $y$ is a logarithm of their bitwise exclusive or (XOR) interpreted as an integer, \ie, $d(x,y) = \textit{log}_2(x \oplus y)$ (\ie the length of differing suffix in bits).

Each node $N$ in the overlay maintains a \emph{routing table} storing a set of peers, each with its \emph{node ID} and reachability information, \ie, IP address and port numbers.
The routing table is partitioned into $m$ \textit{buckets}, numbered zero through $m-1$, where $m$ is a global system parameter.
Bucket $i$ contains a list of up to $k$ peers, whose IDs share a common prefix of length $i$ with $N$.
% The last bucket (\ie, bucket $m-1$) additionally contains peers whose IDs share a prefix of length exceeding $m-1$ bits with the local node. \er{I do not see the point/interest of this precision, this is already covered by the previous sentence.}

% A newly instantiated DHT node typically populates its routing table with a set of (hard-coded) bootstrap nodes\footnote{While other approaches where proposed based on social relations between nodes, currently, most of the deployed DHTs use the bootstrap node approach.}.

\begin{figure}
    \includegraphics[width=0.4\textwidth]{img/kademlia}
    \caption{Organization of the DHT routing table.\mk{Do we need this figure?}}
    \label{fig:kademlia}
 \end{figure}

\smallskip
\noindent
\textbf{Lookups.}
%
The bucket partitioning scheme divides the keyspace from the point of view of $N$ into disjoint intervals, halving in length every time the bucket's associated prefix includes one more common bit with $N$'s ID.
As a result, a node's routing table provides a more detailed (\ie, fine-grained) view of the subset of the network with closer node IDs and a less detailed view of nodes with distant IDs.
This property is essential for efficiency and enables lookup operations that take a logarithmic number of steps (\emph{hops}) in the number of nodes in the network.
It allows, in addition, a degree of flexibility as each bucket can contain \textit{any} peer sampled from those whose IDs fall within the corresponding interval of key space.

Lookups rely on $\alpha$ concurrent routing operations (or \emph{walks}) towards the target key.
In the original Kademlia~\cite{maymounkov2002kademlia}, these lookups specify the target key and collect from each node in the process the $k$ closest nodes to that target, which are fed into the querying node's own routing table.
The process stops when up to $k$ nodes closest to the target are found. %\er{seems contradictory when the goal is to find the exact one that is the closest?}
To enhance the security of lookups, Ethereum nodes provide a distance to a key (instead of the target itself) where distance is defined as the bucket number of the key where 
the target node would be inserted. The XOR metric is then used locally to sort the lookup results, which enables nodes to hide their exact target during lookups, making it harder for adversaries to return favorable results. 
In addition, Ethereum has incorporated several other counter-measures~\cite{marcus2018low, henningsen2019eclipsing} to secure lookups from eclipsing attacks, which we explain in ~\Cref{sec:related}. 


%\er{stopped here, not sure what level of detail we need to the new dht lookup of ethereum dht we need.}
%In Kademlia, the lookup procedure for a target key involves $\alpha$ ``walks'', each searching for the target in parallel. During each walk, the initiator node iteratively selects the closest peer to the target from its routing table and sends the peer a FIND\_NODE query message, specifying the target key. A peer responds to a FIND\_NODE message by returning information on up to $k$ closest nodes (\ie ID, IP, and port) to the given target from its routing table. The initiator node then adds the returned peers to its routing table and iteratively sends another FIND\_NODE message to the closest known peer, extending the traversed path by the walk, until the process converges to a stable $k$ closest known peers to the target.

%\er{need to simplify the following and make it a single paragraph as part of the lookup paragraph}


%Ethereum's DHT implementation uses a rather ``crude'' distance metric allowing nodes to hide their target from other peers in the lookups. Specifically, the distance of a key to a node is the common prefix length of the key and the node's ID, which corresponds to the bucket number of the key where the node would insert the key, following the same bucket partitioning scheme with vanilla Kademlia. In Ethereum's lookup process, the initiating nodes specify the distance of an undisclosed target in their lookup messages rather than the target key itself. Each peer in turn responds with $k$ peers from its bucket whose number corresponds to the supplied distance in the message. In case, there are less than $k$ peers in its bucket, a node can return peers from adjacent buckets. The initiating node terminates the lookup when the process converges to a stable set of (not necessarily $k$) closest nodes based on the modified distance metric, \ie no new peers in the target key's bucket. 



%\oa{briefly mention and reference the eclipsing counter-measures of Ethereum DHT since our protocol assumes the DHT is secure}

\subsection{\discv service discovery}
\label{sec:background:discv4}
In the current service discovery protocol, \discv, nodes perform \textit{random walks} through the DHT and perform ``handshakes'' with the encountered peers.
In a random walk, a node picks a random target key and performs a lookup toward the distance corresponding to the target following the parallel search process explained above. 
A handshake involves a secure channel establishment between the initiator node and the encountered peer and therefore incurs some overhead to both endpoints. If the initiator node discovers during the handshake that the encountered peer is part of a target application's sub-network, then the initiator node can follow up with a connection request to the peer to establish an application-level connection. The objective of a node is to fill up a number of \textit{application-level connection slots} with outbound (\ie initiated by the node) connections. The process of node discovery is suspended when a node has filled up all outbound slots. Nodes also reserve a number of inbound connection slots that can be filled with connections initiated by other nodes. 

\subsection{Security and Efficiency of \discv}
\label{sec:securityEfficiency}
%\mk{The below is too long. I also don't think it's a good place to discuss in more details potential attacks.}
%In the context of blockchains and cryptocurrency, security is a key requirement for a service discovery protocol. A potential attack on any peer discovery protocol is the eclipse attack, where an adversary monopolises all the connections of a victim node with its Sybils, effectively separating the victim from the rest of the network. In blockchain systems, eclipse attacks are typically a pre-cursor to financially-motivated attacks such as double spending and stubborn mining~\cite{henningsen2019eclipsing}.

%Because eclipsing can apply to any peer discovery process, it concerns both the DHT-level (\ie Kademlia routing table) and application-level peer selection. 
Peer discovery through random walks is reasonably resilient against eclipse attacks because attackers can not strategically place their Sybil identities in the keyspace to increase their chance of being discovered by victims, \ie all locations in the keyspace have an equal chance of being discovered.
On the other hand, the brute-force approach of expensive handshakes with randomly encountered nodes has several drawbacks in terms of efficiency. First of all, handshakes with peers not in the target sub-network are wasteful and incur unnecessary overhead. %More importantly, it can take very long to discover peers that are part of an unpopular sub-network (\ie an application for which there is only a small number of peers). 
More importantly, for applications with low popularity, the number of handshakes can be excessive, \ie on the order of thousands when 0.1\% of the nodes are running the target application. Because all the applications are initially unpopular, the upfront cost of building a sub-network can be large and finding peers can take a long time.
